{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hungle45/Workspaces/ias_lab/topics/datn/venv/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import torch\n",
    "\n",
    "from src.datasets.fastspeech_dataset import (\n",
    "    build_path_to_transcript_dict_libri_tts,\n",
    "    FastSpeechDataset,\n",
    "    Miniset,\n",
    ")\n",
    "\n",
    "from src.tts.models.fastspeech2.FastSpeech2 import FastSpeech2\n",
    "from src.pipelines.gst_fastspeech2 import meta_train_loop, train_loop\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_BASE_PATH = '../data/librispeech'\n",
    "# TEST_CLEAN_PATH = os.path.join(DATASET_BASE_PATH, 'test-clean')\n",
    "TEST_CLEAN_PATH = '../data/test-clean'\n",
    "TRAIN_CLEAN_PATH = os.path.join(DATASET_BASE_PATH, 'train-clean-100')\n",
    "ALIGNER_CHECKPOINT = \"../saved_models/aligner.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript_dict = build_path_to_transcript_dict_libri_tts(TEST_CLEAN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared a FastSpeech dataset with 18 datapoints in ./librispeech.\n"
     ]
    }
   ],
   "source": [
    "dataset = FastSpeechDataset(\n",
    "    path_to_transcript_dict=transcript_dict,\n",
    "    acoustic_checkpoint_path=ALIGNER_CHECKPOINT,  # path to aligner.pt\n",
    "    cache_dir=\"./librispeech\",\n",
    "    lang=\"en\",\n",
    "    loading_processes=2,  # depended on how many CPU you have\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_ids = os.listdir(TEST_CLEAN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = FastSpeech2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9 [00:00<?, ?it/s]/home/hungle45/Workspaces/ias_lab/topics/datn/venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "100%|██████████| 9/9 [00:09<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Steps: 1\n",
      "Training Loss: 16.29689619276259 - L1 Loss: 4.165504720475939 - Duration Loss: 3.2045013109842935 - Pitch Loss: 4.375345044665867 - Energy Loss: 4.551544480853611 - Cycle Loss: 2.284200734562344\n",
      "Time elapsed:  0 Minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:07<00:00,  1.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Steps: 2\n",
      "Training Loss: 13.451093461778429 - L1 Loss: 2.7299438847435846 - Duration Loss: 2.217226915889316 - Pitch Loss: 4.0048430230882435 - Energy Loss: 4.499079492357042 - Cycle Loss: 2.170311450958252\n",
      "Time elapsed:  0 Minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:07<00:00,  1.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Steps: 3\n",
      "Training Loss: 11.108128229777018 - L1 Loss: 2.0205891132354736 - Duration Loss: 1.6857527494430542 - Pitch Loss: 4.119923114776611 - Energy Loss: 3.2818632390764026 - Cycle Loss: 2.000499341222975\n",
      "Time elapsed:  0 Minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:07<00:00,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Steps: 4\n",
      "Training Loss: 9.522344165378147 - L1 Loss: 1.8781116141213312 - Duration Loss: 1.5366687112384372 - Pitch Loss: 3.371674749586317 - Energy Loss: 2.7358892493777804 - Cycle Loss: 1.7321425676345825\n",
      "Time elapsed:  0 Minutes\n"
     ]
    }
   ],
   "source": [
    "train_loop.train_loop(\n",
    "    net,\n",
    "    dataset,\n",
    "    device=device,\n",
    "    batch_size=2,\n",
    "    save_directory=\"./save_models\",\n",
    "    path_to_checkpoint=None,\n",
    "    resume=False,\n",
    "    phase_1_steps=2,\n",
    "    phase_2_steps=2,\n",
    "    steps_per_save=1,\n",
    "    path_to_embed_model=\"../saved_models/embedding_function.pt\",\n",
    "    lr=0.01,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
