{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from IPython.display import Audio\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "from src.spk_embedding.StyleEmbedding import StyleEmbedding\n",
    "from src.tts.vocoders.hifigan.HiFiGAN import HiFiGANGenerator\n",
    "from src.tts.models.fastporta.FastPorta import FastPorta\n",
    "from src.datasets.fastspeech_dataset import (\n",
    "    FastSpeechDataset,\n",
    "    build_path_to_transcript_dict_libri_tts,\n",
    ")\n",
    "from src.pipelines.fastporta.train_loop import collate_and_pad\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_CLEAN_PATH = '../data/test-clean'\n",
    "\n",
    "AVOCODO_CHECKPOINT = \"../saved_models/Avocodo.pt\"\n",
    "ALIGNER_CHECKPOINT = \"../saved_models/aligner.pt\"\n",
    "FASTPORTA_CHECKPOINT = \"../saved_models/fastporta/checkpoint_lastest.pt\"\n",
    "STYLE_EMBED_CHECKPOINT = \"../saved_models/embedding_function.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript_dict = build_path_to_transcript_dict_libri_tts(TEST_CLEAN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = FastSpeechDataset(\n",
    "    path_to_transcript_dict=transcript_dict,\n",
    "    acoustic_checkpoint_path=ALIGNER_CHECKPOINT,  # path to aligner.pt\n",
    "    cache_dir=\"./librispeech\",\n",
    "    lang=\"en\",\n",
    "    loading_processes=2,  # depended on how many CPU you have\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocoder = HiFiGANGenerator().to(device)\n",
    "avocodo_check_dict = torch.load(AVOCODO_CHECKPOINT, map_location=device)\n",
    "vocoder.load_state_dict(avocodo_check_dict[\"generator\"])\n",
    "vocoder.eval()\n",
    "\n",
    "style_embed_function = StyleEmbedding().to(device)\n",
    "style_embed_check_dict = torch.load(STYLE_EMBED_CHECKPOINT, map_location=device)\n",
    "style_embed_function.load_state_dict(style_embed_check_dict[\"style_emb_func\"])\n",
    "style_embed_function.eval()\n",
    "style_embed_function.requires_grad_(False)\n",
    "\n",
    "acoustic_model = FastPorta().to(device)\n",
    "fastspeech2_check_dict = torch.load(FASTPORTA_CHECKPOINT, map_location=device)\n",
    "acoustic_model.load_state_dict(fastspeech2_check_dict[\"model\"])\n",
    "acoustic_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_id = 1\n",
    "sample = dataset[sample_id]\n",
    "input_audio_path = sample[-1]\n",
    "input_wave, _ = librosa.load(input_audio_path)\n",
    "input_text = transcript_dict[input_audio_path]\n",
    "batch = collate_and_pad([sample])\n",
    "\n",
    "print(input_audio_path)\n",
    "Audio(data=input_wave, rate=24000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_embedding = style_embed_function(\n",
    "    batch_of_spectrograms=batch[2].to(device),\n",
    "    batch_of_spectrogram_lengths=batch[3].to(device),\n",
    ")\n",
    "\n",
    "mel = acoustic_model.inference(\n",
    "    text=batch[0][0].to(device),\n",
    "    speech=None,\n",
    "    alpha=1.0,\n",
    "    utterance_embedding=style_embedding[0],\n",
    "    return_duration_pitch_energy=False,\n",
    "    lang_id=batch[8][0].to(device),\n",
    ")\n",
    "\n",
    "waveform = vocoder(mel.transpose(1, 0))[0]\n",
    "waveform = waveform.detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mel.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(input_text)\n",
    "Audio(data=waveform, rate=24000, autoplay=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torchaudio.save(\n",
    "#     'synth.wav',\n",
    "#     src=waveform,\n",
    "#     sample_rate=16000\n",
    "# )\n",
    "\n",
    "# torchaudio.save(\n",
    "#     'origin.wav',\n",
    "#     src=torch.Tensor(input_wave).unsqueeze(0),\n",
    "#     sample_rate=16000\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
