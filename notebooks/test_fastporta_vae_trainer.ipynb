{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "import librosa\n",
    "import numpy as np\n",
    "from numpy import trim_zeros\n",
    "\n",
    "\n",
    "from src.datasets.fastspeech_dataset import (\n",
    "    build_path_to_transcript_dict_libri_tts,\n",
    "    FastSpeechDataset)\n",
    "from src.tts.models.fastporta.FastPortaVAE import FastPortaVAE\n",
    "from src.pipelines.fastporta_vae.train_loop import train_loop\n",
    "from src.spk_embedding.StyleEmbedding import StyleEmbedding\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_CLEAN_PATH = '../data/test-clean'\n",
    "ALIGNER_CHECKPOINT = '../saved_models/aligner.pt'\n",
    "EMBED_MODEL = \"../saved_models/embedding_function.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript_dict = build_path_to_transcript_dict_libri_tts(TEST_CLEAN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared a FastSpeech dataset with 10 datapoints in ./librispeech.\n"
     ]
    }
   ],
   "source": [
    "dataset = FastSpeechDataset(\n",
    "    path_to_transcript_dict=transcript_dict,\n",
    "    acoustic_checkpoint_path=ALIGNER_CHECKPOINT,  # path to aligner.pt\n",
    "    cache_dir=\"./librispeech\",\n",
    "    lang=\"en\",\n",
    "    loading_processes=2,  # depended on how many CPU you have\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = FastPortaVAE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nmmy/Documents/src/DATN-VoiceCloning/venv/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py:120: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]/home/nmmy/Documents/src/DATN-VoiceCloning/venv/lib/python3.10/site-packages/torch/amp/autocast_mode.py:204: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
      "100%|██████████| 5/5 [01:25<00:00, 17.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Steps: 1\n",
      "Train Loss: 11.680 - Mel Loss: 2.389 - Glow Loss: 3.566 - Duration Loss: 2.453 - Pitch Loss: 1.210 - Energy Loss: 1.694 - Cycle Loss: 2.631 - KL Loss: 0.041 - VAE Reconstruction Loss: 0.327\n",
      "Time elapsed:  1 Minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [01:32<00:00, 18.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Steps: 2\n",
      "Train Loss: 10.395 - Mel Loss: 2.287 - Glow Loss: 3.499 - Duration Loss: 1.592 - Pitch Loss: 1.141 - Energy Loss: 1.508 - Cycle Loss: 2.637 - KL Loss: 0.039 - VAE Reconstruction Loss: 0.327\n",
      "Time elapsed:  2 Minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [01:48<00:00, 21.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Steps: 3\n",
      "Train Loss: 9.771 - Mel Loss: 2.099 - Glow Loss: 3.178 - Duration Loss: 1.418 - Pitch Loss: 1.139 - Energy Loss: 1.570 - Cycle Loss: 2.646 - KL Loss: 0.037 - VAE Reconstruction Loss: 0.329\n",
      "Time elapsed:  2 Minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [01:22<00:00, 16.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Steps: 4\n",
      "Train Loss: 8.611 - Mel Loss: 1.909 - Glow Loss: 2.683 - Duration Loss: 1.359 - Pitch Loss: 0.984 - Energy Loss: 1.315 - Cycle Loss: 2.606 - KL Loss: 0.035 - VAE Reconstruction Loss: 0.326\n",
      "Time elapsed:  2 Minutes\n"
     ]
    }
   ],
   "source": [
    "train_loop(\n",
    "    net,\n",
    "    dataset,\n",
    "    device=device,\n",
    "    batch_size=2,\n",
    "    save_directory=\"../saved_models\",\n",
    "    path_to_checkpoint=None,\n",
    "    resume=False,\n",
    "    phase_1_steps=4,\n",
    "    phase_2_steps=0,\n",
    "    steps_per_save=1,\n",
    "    path_to_embed_model=\"../saved_models/embedding_function.pt\",\n",
    "    lr=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tensors=torch.rand(2,5,62)\n",
    "text_lens=torch.tensor([5,3])\n",
    "speech_lens=torch.tensor([10,7])\n",
    "gold_speech=torch.rand(2,10,80)\n",
    "gold_durations=torch.tensor([\n",
    "    [2,2,2,2,2],\n",
    "    [2,2,3,0,0]\n",
    "])\n",
    "gold_pitch=torch.rand(2,5,1)\n",
    "gold_energy=torch.rand(2,5,1)\n",
    "is_inference=False\n",
    "alpha=1.0\n",
    "utterance_embedding=torch.rand(2,64)\n",
    "lang_ids=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 10, 80])\n",
      "torch.Size([2, 5])\n",
      "torch.Size([2, 5, 1])\n",
      "torch.Size([2, 5, 1])\n",
      "None\n",
      "torch.Size([])\n",
      "torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "outs = net._forward(\n",
    "    text_tensors=text_tensors,\n",
    "    text_lens=text_lens,\n",
    "    utterance_embedding=utterance_embedding,\n",
    "    is_inference=True,\n",
    "    lang_ids=lang_ids,\n",
    ")\n",
    "\n",
    "for v in outs:\n",
    "    print(v.shape if v != None else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 10, 80])\n",
      "torch.Size([2, 5])\n",
      "torch.Size([2, 5, 1])\n",
      "torch.Size([2, 5, 1])\n",
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "outs = net._forward(\n",
    "    text_tensors=text_tensors,\n",
    "    text_lens=text_lens,\n",
    "    gold_speech=gold_speech,\n",
    "    speech_lens=speech_lens,\n",
    "    gold_durations=gold_durations,\n",
    "    gold_pitch=gold_pitch,\n",
    "    gold_energy=gold_energy,\n",
    "    utterance_embedding=utterance_embedding,\n",
    "    is_inference=False,\n",
    "    lang_ids=lang_ids,\n",
    ")\n",
    "\n",
    "for v in outs:\n",
    "    print(v.shape if v != None else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor(4.4390), tensor(1.1356), tensor(1.0728), tensor(0.4119), tensor(0.7976), tensor(0.6318), tensor(0.0305), tensor(0.3588))\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outs = net.forward(\n",
    "        text_tensors=text_tensors,\n",
    "        text_lengths=text_lens,\n",
    "        gold_speech=gold_speech,\n",
    "        speech_lengths=speech_lens,\n",
    "        gold_durations=gold_durations,\n",
    "        gold_pitch=gold_pitch,\n",
    "        gold_energy=gold_energy,\n",
    "        utterance_embedding=utterance_embedding,\n",
    "        lang_ids=lang_ids,\n",
    "    )\n",
    "\n",
    "    print(outs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
